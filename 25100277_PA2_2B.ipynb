{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA2.2 Part B - Hidden Markov Model (HMM) Based Music Generator\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, you will be generating Music (no vocals though) using an HMM via [Baum-Welch](https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm) Training Algorithm.\n",
    "\n",
    "In the context of Music Generation, the states might represent underlying musical concepts (like note pitches or chord types), and observations could be specific notes or chords played at a given time. Hence, generating music involves moving through the states based on transition probabilities and producing musical notes based on the emission probabilities associated with each state. The emission probabilities dictate how likely it is to observe each possible note or chord (observation symbol) when in a given state.\n",
    "\n",
    "## Terminology\n",
    "\n",
    "__Baum Welch__: is an __*Unsupervised*__ training algorithm that involves adjusting the HMM's parameters (transition, emission, and initial state probabilities) to best account for the observed sequences.The training process involves:\n",
    "- Expectation step (E-step): Estimate the likely sequences of hidden states (could be something implicit like musical concepts like chords or rhythms) given the current parameters of the model and the observed data.\n",
    "- Maximization step (M-step): Update the model's parameters to maximize the likelihood of the observed data, based on the estimated sequences of hidden states.\n",
    "\n",
    "![Baum Welch](unsupervised_learning.png)\n",
    "\n",
    "## Resources\n",
    "\n",
    "For additional details of the working of Baum-Welch Training you can consult these medium articles [Baum-Welch algorithm](https://medium.com/mlearning-ai/baum-welch-algorithm-4d4514cf9dbe) and [Baum-Welch algorithm for training a Hidden Markov Model — Part 2 of the HMM series](https://medium.com/analytics-vidhya/baum-welch-algorithm-for-training-a-hidden-markov-model-part-2-of-the-hmm-series-d0e393b4fb86) as reference.\n",
    "\n",
    "A more technical overview is covered by Rabiner in his paper on [A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition](http://www.stat.columbia.edu/~liam/teaching/neurostat-fall17/papers/hmm/rabiner.pdf).\n",
    "\n",
    "If the above link is a bit difficult to digest, you can consult the following slides by Stanford's Dan Jurafsky in his course [LSA 352: Speech Recognition and Synthesis](https://nlp.stanford.edu/courses/lsa352/lsa352.lec7.6up.pdf).\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Follow along with the notebook, filling out the necessary code where instructed.\n",
    "\n",
    "- <span style=\"color: red;\">Read the Submission Instructions, Plagiarism Policy, and Late Days Policy in the attached PDF.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Make sure to run all cells for credit.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Do not remove any pre-written code.</span>\n",
    "\n",
    "- <span style=\"color: red;\">You must attempt all parts.</span>\n",
    "\n",
    "For this notebook, in addition to standard libraries i.e. `numpy`, `tqdm`, `hmmlearn` and `muspy`, you are permitted to incorporate supplementary libraries, but it is strongly advised to restrict their inclusion to a minimum. However, other HMM toolkits or libraries are strictly prohibited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip downloading as the MuseScore General soundfont is found.\n"
     ]
    }
   ],
   "source": [
    "#!pip install muspy\n",
    "#!pip install hmmlearn\n",
    "import muspy\n",
    "muspy.download_musescore_soundfont()\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from hmmlearn.hmm import CategoricalHMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MusPy** is an open source Python library for symbolic music generation. It provides essential tools for developing a music generation system, including dataset management, data I/O,  \n",
    "data preprocessing and model evaluation.  \n",
    "**Documentation**: https://salu133445.github.io/muspy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'muspy.datasets.emopia.EMOPIADataset'>, <class 'muspy.datasets.essen.EssenFolkSongDatabase'>, <class 'muspy.datasets.haydn.HaydnOp20Dataset'>, <class 'muspy.datasets.hymnal.HymnalDataset'>, <class 'muspy.datasets.hymnal.HymnalTuneDataset'>, <class 'muspy.datasets.jsb.JSBChoralesDataset'>, <class 'muspy.datasets.lmd.LakhMIDIAlignedDataset'>, <class 'muspy.datasets.lmd.LakhMIDIDataset'>, <class 'muspy.datasets.lmd.LakhMIDIMatchedDataset'>, <class 'muspy.datasets.maestro.MAESTRODatasetV1'>, <class 'muspy.datasets.maestro.MAESTRODatasetV2'>, <class 'muspy.datasets.maestro.MAESTRODatasetV3'>, <class 'muspy.datasets.music21.Music21Dataset'>, <class 'muspy.datasets.musicnet.MusicNetDataset'>, <class 'muspy.datasets.nes.NESMusicDatabase'>, <class 'muspy.datasets.nmd.NottinghamDatabase'>, <class 'muspy.datasets.wikifonia.WikifoniaDataset'>]\n"
     ]
    }
   ],
   "source": [
    "#List of available datasets in muspy (could also add your own datasets)\n",
    "#Link: https://salu133445.github.io/muspy/datasets/datasets.html\n",
    "print(muspy.list_datasets())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Baum-Welch is known for its slow convergence, we'll take the lightest dataset available from muspy datasets called the __HaydnOp20__ Dataset consisting of 1.26 hours of recordings \\\n",
    "comprising of 24 classical songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip downloading as the `.muspy.success` file is found.\n",
      "Skip extracting as the `.muspy.success` file is found.\n",
      "Skip conversion as the `.muspy.success` file is found.\n"
     ]
    }
   ],
   "source": [
    "my_dataset = muspy.datasets.HaydnOp20Dataset(root = 'C:/Users/user/.muspy/musescore-general', download_and_extract=True)\n",
    "my_dataset = my_dataset.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: (HMM + Baum Welch From Scratch) [80 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muspy offers all datasets in 4 different representations as mentioned below: \n",
    "\n",
    "![Alt text](muspy_representations.png)\n",
    "\n",
    "Initially, we are only interested in modelling through time and to keep it simple, we'll begin with the __Pitch Representation__. More details here:\n",
    "\n",
    "![text](muspy_to_pitch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_collection = []\n",
    "for music in my_dataset:\n",
    "    music_collection.append(muspy.to_pitch_representation(music, use_hold_state=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singing HMM Class\n",
    "The __Singing_HMM__ Class contains the following methods:\n",
    "\n",
    "1. `__init__(self, corpus)`: initializes the __POS_HMM__ and prepares it for the parameter initialization phase, contains:\n",
    "    - a corpus, consisting of unlabeled  sequences of musical units (i.e. all the music songs are flattened and concatenated)\n",
    "    - a hidden_state_size (default to 10), higher values capture more variability but converge slowly.\n",
    "    - a tuple of all the unique \n",
    "    - a dictionary for mapping the pitches to its unique integer identifier.\n",
    "    - some additional variables to reduce code redundancy in latter parts such as len()\n",
    "    - Transition, Emission and Initial State Probability Matrices which are initialized to Zeros.\n",
    "\n",
    "2. `init_mat(self, init_scheme='uniform')`: __(Can Be Modified)__ initializes the transition, emission and probability matrices either with a 'uniform' value or values sampled randomly from a uniform distribution and normalizes the matrice row wise.\n",
    "\n",
    "3. `forward(self, sequence)`: __(To Be Implemented)__ implements the Forward stage of the Forward-Backward Algorithm. \n",
    "- Feel free to modify function signature and return values.\n",
    "- Do not change the function name.\n",
    "4. `backward(self, sequence)`: __(To Be Implemented)__ implements the Forward stage of the Forward-Backward Algorithm. \n",
    "- Feel free to modify function signature and return values.\n",
    "- Do not change the function name.\n",
    "6. `baum_welch(sequence, alpha, beta)`: __(To Be Implemented)__ implements the Baum Welch Training Algorithm. \n",
    "- Feel free to modify function signature and return values.\n",
    "- Do not change the function name.\n",
    "7. `softmax(self, x, temperature=1.0)`: calculates the softmax of a given input x adjusting the sharpness of the distribution based on a temperature parameter.\n",
    "\n",
    "8. `temperature_choice(self, probabilities, temperature=1.0)`: applies a temperature scaling to a set of probabilities and selects an index based on the adjusted probabilities.\n",
    "\n",
    "9. `sample_sequence(self, length, strategy = \"temperature\", temperature = 1.0)`: __(Can Be Modified)__ generates a sequence of elements based on a given strategy (probabilistic or temperature) and a specified length. Strategies consists of:\n",
    "* `probabilistic` strategy:\n",
    "    -  Samples the initial state based on initial state probabilities.\n",
    "    -  Iterates over the desired sequence length, sampling an observation based on the current state's emission probabilities, appending the observation to the sequence, and then transitioning to the next state based on the current state's transition probabilities.\n",
    "* `temperature` strategy:\n",
    "    -  Similar to the probabilistic strategy but applies temperature scaling to the choice of initial state, observation sampling, and state transitions to adjust the randomness of the choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __READ THIS BEFORE YOU BEGIN__:\n",
    "- The functions `init_mat` and  `sample_sequence` are although pre-defined and will work properly, but if you have a better strategy feel free to add or experiment. Just make sure not to overwrite the pre-existing code.\n",
    "- You are allowed to make helper functions, just make sure they are neatly structured i.e. have self-explanatory variable names, have an algorithmic flow to them and are sufficiently commented.\n",
    "- Make sure not to change any exisiting code unless allowed by the TA.\n",
    "\n",
    "__Tips for Baum-Welch Implementation__:\n",
    "\n",
    "1. Write the code for simple/vanilla Baum Welch Implementation first.\n",
    "2. You have the option to either go over the whole concatenated sequence or each music seperately (in a nested for loop) per iteration.\n",
    "3. If your vanilla Baum Welch Implementation compiles, most likely you would get overflow errors, arising from division by 0. This is due to long sequences yielding  \\\n",
    "smaller values of alpha and beta. Hence, wherever division occurs, the denominator variable (which is a result of multiplication with alpha or beta) is close to 0.\n",
    "\n",
    "I'll now suggest some ways with which the third point can be alleviated __(the hacky ways might/might not work, so be wary)__:\n",
    "\n",
    "- Hacky way #1 (Working with smaller chunks of observed sequences): For every iteration, rather than going over the concatenated music sequences or each music sequence, you can further break down your musical sequences into even smaller chunks and go over those instead.\n",
    "\n",
    "- Hacky way #2 (Add a small epsilon value to the denominator): Add a small episilon value like 1e-12 to the denominator wherever the division by 0 error occurs. \n",
    "\n",
    "- Proper way #1 (The [log-sum-exp](https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/) trick): For an HMM, the smaller values can be dealt with by passing them through\n",
    "    log and converting the multiplications to additions and then brought back via exponentiating them.\n",
    "\n",
    "    - Another [intro](https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/) for the log-sum-exp, if the previous one was unclear.\n",
    "    - [Hidden Markov Models By Herman Kemper](https://www.kamperh.com/nlp817/notes/05_hmm_notes.pdf) illustrates the use of log-sum-exp technique in Baum Welch Implementation (particularly Forward and Backward Passes).\n",
    "    - [Recition 7: Hidden Markov Models](https://www.cs.cmu.edu/~mgormley/courses/10601-s23/handouts/hw7_recitation_solution.pdf) gives an idea of the usage of log-sum-exp in the forward-backward algorithm.\n",
    "    - This HMM github [repo](https://github.com/jwmi/HMM/blob/master/hmm.jl) has implemented the log-sum-exp trick in julia language.\n",
    "    - The following [blog post](https://gregorygundersen.com/blog/2020/11/28/hmms/#implementation) might also be helpful for implementation of baum-welch using log-sum-exp trick.\n",
    "    - The following paper on [Numerically Stable Hidden Markov Models](http://bozeman.genome.washington.edu/compbio/mbt599_2006/hmm_scaling_revised.pdf) gives pseudocodes for working in the log domain for the HMMs (although not necessarily the log-sum-exp trick as is).\n",
    "\n",
    "- Proper way #2 (Scaling Factors): involves scaling the alpha and beta values to avoid underflows.\n",
    "    - The following blog post explains the maths behind scaling [Scaling Factors for Hidden Markov Models](https://gregorygundersen.com/blog/2022/08/13/hmm-scaling-factors/)\n",
    "    - This stackexchange post [Scaling step in Baum-Welch algorithm](https://stats.stackexchange.com/questions/274175/scaling-step-in-baum-welch-algorithm) contains two answers which can also be consulted.\n",
    "\n",
    "__How do you know the HMM is converging?__:\n",
    "\n",
    "Since Baum Welch algorithm guarantees convergence to the local (not global) maxima, near zero values are difficult to achieve.  \\\n",
    "Hence, a convergening HMM would have the log likelihoods going towards 0 (although still far from it). You can find a sample cell output  \\\n",
    "below showing the log likelihoods decreasing. Another way is to see is that the post-convergence generated music would be better than the  \\\n",
    "starting HMM (which has uniform or randomly initialized matrices).\n",
    "\n",
    "__How do you know the HMM has converged?__:\n",
    "\n",
    "One way is to monitor the difference between two successive log likelihoods and stop when the differences goes below a certain threshold. This has already been implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   scipy.special import logsumexp\n",
    "from   scipy.stats import multivariate_normal as mvn\n",
    "class Singing_HMM:\n",
    "    def __init__(self, corpus, hidden_state_size=10):\n",
    "        self.corpus = [seq.flatten().tolist() for seq in corpus]\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        self.music_seq = [note for seq in self.corpus for note in seq]\n",
    "        self.vocab = tuple(set(self.music_seq))\n",
    "        self.vocab2index = {note: i for i, note in enumerate(self.vocab)}\n",
    "        self.vocab_len = len(self.vocab)\n",
    "        \n",
    "        self.transition_mat = np.zeros((self.hidden_state_size, self.hidden_state_size))\n",
    "        self.emission_mat = np.zeros((self.hidden_state_size, self.vocab_len))\n",
    "        self.initial_state_prob = np.zeros(self.hidden_state_size)\n",
    "\n",
    "        #----------------Add Any Additional Code Below This Line----------------\n",
    "\n",
    "\n",
    "    #Feel free to define any helper functions\n",
    "        \n",
    "    def init_mat(self, init_scheme='uniform'): # Can be optionally modified for another initialization scheme (not necessary for the assignment)\n",
    "        if init_scheme == 'uniform':\n",
    "            self.transition_mat = np.ones((self.hidden_state_size, self.hidden_state_size))\n",
    "            self.emission_mat = np.ones((self.hidden_state_size, self.vocab_len))\n",
    "            self.initial_state_prob = np.ones(self.hidden_state_size)\n",
    "        elif init_scheme == 'random':\n",
    "            self.transition_mat = np.random.rand(self.hidden_state_size, self.hidden_state_size)\n",
    "            self.emission_mat = np.random.rand(self.hidden_state_size, self.vocab_len)\n",
    "            self.initial_state_prob = np.random.rand(self.hidden_state_size)\n",
    "        \n",
    "        self.transition_mat /= self.transition_mat.sum(axis=1, keepdims=True)\n",
    "        self.emission_mat /= self.emission_mat.sum(axis=1, keepdims=True)\n",
    "        self.initial_state_prob /= self.initial_state_prob.sum()\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        \"\"\"\n",
    "        Forward algorithm for calculating the probabilities of a sequence.\n",
    "        \"\"\"\n",
    "        alpha = None\n",
    "        obs = sequence\n",
    "        N = len(obs)\n",
    "\n",
    "        log_alpha = np.zeros((N, self.hidden_state_size))\n",
    "\n",
    "        for k in range(self.hidden_state_size):\n",
    "            log_alpha[0, k] = np.log(self.initial_state_prob[k]) + self.emission_mat[k, self.vocab2index[obs[0]]]\n",
    "\n",
    "        for n in range(1, N):\n",
    "            for k in range(self.hidden_state_size):\n",
    "                tmp = np.empty(self.hidden_state_size)\n",
    "                for j in range(self.hidden_state_size):\n",
    "                    tmp[j] = log_alpha[n-1, j] + self.transition_mat[j, k]  # Perform calculation in log space\n",
    "                log_alpha[n, k] = logsumexp(tmp) + self.emission_mat[k, self.vocab2index[obs[n]]]\n",
    "\n",
    "\n",
    "        return log_alpha\n",
    "\n",
    "    def backward(self, sequence):\n",
    "        \"\"\"\n",
    "        Backward algorithm for calculating the probabilities of a sequence.\n",
    "        \"\"\"\n",
    "        beta = None\n",
    "        obs = sequence\n",
    "        N = len(obs)\n",
    "\n",
    "        log_beta = np.zeros((N, self.hidden_state_size))\n",
    "\n",
    "        log_beta[N-1] = 0  # log(1)\n",
    "        for n in reversed(range(N-1)):\n",
    "            for k in range(self.hidden_state_size):\n",
    "                tmp = np.empty(self.hidden_state_size)\n",
    "                for j in range(self.hidden_state_size):\n",
    "                    tmp[j] = (log_beta[n+1, j] + self.emission_mat[j, self.vocab2index[obs[n+1]]] + self.transition_mat[k, j])  # Perform calculation in log space\n",
    "                log_beta[n, k] = logsumexp(tmp)\n",
    "\n",
    "        return log_beta\n",
    "    \n",
    "    def baum_welch(self, n_iter=100, tol=1e-4):\n",
    "        \"\"\"\n",
    "        Perform Baum-Welch training to update the model's parameters.\n",
    "        \"\"\"\n",
    "        prev_log_likelihood = float('-inf')  # Initialize with negative infinity (DO NOT CHANGE THIS VARIABLE)\n",
    "\n",
    "        for iteration in tqdm(range(n_iter), desc=\"Training Progress\", leave=True):\n",
    "            log_likelihood = 0 # Log likelihood for this iteration (DO NOT CHANGE THIS VARIABLE)\n",
    "            #----------------Add Your Code Here----------------\n",
    "            for sequence in self.corpus:\n",
    "                obs = sequence\n",
    "                N = len(obs)\n",
    "\n",
    "                # Expectation step (forward-backward algorithm)\n",
    "                log_alpha = np.zeros((N, self.hidden_state_size))\n",
    "                log_beta = np.zeros((N, self.hidden_state_size))\n",
    "\n",
    "                log_alpha = self.forward(sequence)\n",
    "                log_beta = self.backward(sequence)\n",
    "\n",
    "                # M-step.\n",
    "                # Compute first posterior moment, \\gamma (size N × K).\n",
    "                log_gamma = log_alpha + log_beta\n",
    "                log_evidence = logsumexp(log_alpha[N-1])\n",
    "                gamma = np.exp(log_gamma - log_evidence)\n",
    "\n",
    "                # Compute second posterior moment, \\xi (size N × K × K).\n",
    "                log_xi = np.empty((N, self.hidden_state_size, self.hidden_state_size))\n",
    "                for n in range(N-1):\n",
    "                    tmp = np.empty((self.hidden_state_size, self.hidden_state_size))\n",
    "                    for k in range(self.hidden_state_size):\n",
    "                        for j in range(self.hidden_state_size):\n",
    "                            tmp[k, j] = (log_alpha[n, k]+ log_beta[n+1, j] + self.emission_mat[j, self.vocab2index[obs[n+1]]] + self.transition_mat[k, j]- log_evidence)\n",
    "                    log_xi[n] = tmp\n",
    "\n",
    "                # Update initial state probabilities (pi)\n",
    "                log_pi = log_gamma[0] - logsumexp(log_gamma[0])\n",
    "                assert log_pi.size == self.hidden_state_size\n",
    "                self.initial_state_prob = np.exp(log_pi)\n",
    "\n",
    "                # Update transition probabilities (A)\n",
    "                for i in range(self.hidden_state_size):\n",
    "                    for j in range(self.hidden_state_size):\n",
    "                        self.transition_mat[i, j] = logsumexp(log_xi[1:, i, j]) - logsumexp(log_xi[1:, i, :])\n",
    "\n",
    "                # update emission probabilities\n",
    "                for k in range(self.hidden_state_size):\n",
    "                  for v in self.vocab:\n",
    "                      self.emission_mat[k, self.vocab2index[v]] = np.sum(gamma[obs == v, k]) / np.sum(gamma[:, k])\n",
    "\n",
    "                # Calculate log likelihood\n",
    "                log_likelihood += log_evidence\n",
    "\n",
    "\n",
    "            prev_log_likelihood = log_likelihood\n",
    "            \n",
    "\n",
    "            #----------------Do Not Modify The Code Below This Line----------------\n",
    "            if iteration == 0:\n",
    "                convergence_rate = convergence_diff = np.nan  # Print nan for the first iteration\n",
    "            else:\n",
    "                convergence_diff = np.abs(log_likelihood - prev_log_likelihood)\n",
    "                convergence_rate = convergence_diff / np.abs(prev_log_likelihood)\n",
    "            \n",
    "            #Note that Log Likelihoods would be negative and would increase (i.e. go in the direction of 0) as the model converges.\n",
    "            # Log Likelihoods may be far from 0, but the increasing trend should remain present.\n",
    "            tqdm.write(f\"Iteration {iteration + 1}: Log Likelihood: {log_likelihood}, Convergence Difference: {convergence_diff} , Convergence Rate: {convergence_rate}\")\n",
    "            \n",
    "            if iteration > 0 and convergence_rate < tol:\n",
    "                tqdm.write(\"Convergence achieved.\")\n",
    "                break\n",
    "            \n",
    "            prev_log_likelihood = log_likelihood\n",
    "        \n",
    "        # normalize\n",
    "        # Normalization of self.transition_mat and self.emission_mat (after EM step)\n",
    "        self.transition_mat = np.exp(self.transition_mat)  # Exponentiate transition_mat\n",
    "        self.transition_mat /= np.sum(self.transition_mat, axis=1, keepdims=True)  # Normalize by row\n",
    "\n",
    "        self.emission_mat = np.exp(self.emission_mat)  # Exponentiate emission_mat\n",
    "        self.emission_mat /= np.sum(self.emission_mat, axis=1, keepdims=True)  # Normalize by row\n",
    "\n",
    "\n",
    "\n",
    "    def softmax(self, x, temperature=1.0):\n",
    "        '''Compute softmax values for each set of scores in x.'''\n",
    "        e_x = np.exp((x - np.max(x)) / temperature)\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    def temperature_choice(self, probabilities, temperature=1.0):\n",
    "        '''Apply temperature to probabilities and make a choice.'''\n",
    "        adjusted_probs = self.softmax(np.log(probabilities + 1e-9), temperature)  # Adding epsilon to avoid log(0)\n",
    "        return np.random.choice(len(probabilities), p=adjusted_probs)\n",
    "                        \n",
    "    def sample_sequence(self, length, strategy = \"temperature\", temperature = 1.0):\n",
    "        sequence = []\n",
    "        if strategy == 'probabilistic':\n",
    "            # Sample the initial state\n",
    "            state = np.random.choice(self.hidden_state_size, p=self.initial_state_prob)\n",
    "            for _ in range(length):\n",
    "                # Sample an observation (note) based on the current state\n",
    "                note = np.random.choice(self.vocab, p=self.emission_mat[state])\n",
    "                sequence.append(note)\n",
    "                # Transition to the next state\n",
    "                state = np.random.choice(self.hidden_state_size, p=self.transition_mat[state])\n",
    "        elif strategy == 'temperature':\n",
    "            # Sample the initial state with temperature\n",
    "            state = self.temperature_choice(self.initial_state_prob, temperature)\n",
    "            for _ in range(length):\n",
    "                # Apply temperature to emission probabilities and sample a note\n",
    "                note = self.temperature_choice(self.emission_mat[state], temperature)\n",
    "                sequence.append(self.vocab[note])\n",
    "                # Transition to the next state with temperature\n",
    "                state = self.temperature_choice(self.transition_mat[state], temperature)\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bb90d51b874e4aa3300cc218451928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Log Likelihood: 19092.091271822974, Convergence Difference: nan , Convergence Rate: nan\n",
      "Iteration 2: Log Likelihood: -2.0594637106796654e-12, Convergence Difference: 0.0 , Convergence Rate: 0.0\n",
      "Convergence achieved.\n"
     ]
    }
   ],
   "source": [
    "#Specify values and run the code to test your implementation\n",
    "pos_hmm = Singing_HMM(corpus = music_collection, hidden_state_size = 5)\n",
    "pos_hmm.init_mat(init_scheme = 'random') #Note: HMMs are sensitive to intialization schemes\n",
    "pos_hmm.baum_welch(tol = 1e-5, n_iter= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_seq = pos_hmm.sample_sequence(1024, strategy= \"probabilistic\") #Feel free to experiment with the sampling strategy\n",
    "synthetic_music = muspy.from_pitch_representation(np.array(notes_seq), resolution=24, program=0, is_drum=False, use_hold_state=True, default_velocity=64)\n",
    "muspy.write_midi('C:/LUMS/Spring 23-24/GenAI/PA2.2/pitch_based.mid', synthetic_music) #Specify the path to save the MIDI file, name it \"pitch_based.mid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize your results here: https://cifkao.github.io/html-midi-player/  \n",
    "  \n",
    "Remember to brag about your generated music on Slack (You can use online __MIDI to WAV/MP3__ Converters)\n",
    "\n",
    "__*P.S*__: You can use muspy.write_audio to convert the music object directly to wav file but that requires installation of a few softwares (not worth the hassle).\n",
    "\n",
    "*You might notice that the results are although better than random but they are not as awe-inspiring as intended.\n",
    "The reason being that our model is unable to capture the  \\\n",
    "variability of the different music styles (our dataset comprises of). However, there is a way to generate better music, that is taking a sufficiently long MIDI  \\\n",
    "(could be other formats as well) sound track(s) of a single artist (EDM or any music which has repetitiveness in it) and refitting your HMM.  \\\n",
    "The relevent function here would be muspy.read_midi().  \\\n",
    "__After training on the notebook provided dataset, you are more than welcome to try it with your own curated dataset and see the results.  \\\n",
    "THIS IS OPTIONAL AND NOT MANDATORY__.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Synthetic Music Generation via __HMMLearn__ [20 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Note:__ For any model that you train/fit, remember to set __verbose = True__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = 32 #Number of hidden states in the HMM model (Feel free to change or experiment with this value)\n",
    "sythetic_music_sequence_length = 128 #Length of the synthetic music sequence to be generated (could be either a time step, an event or a note)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, let's replicate what we did above manually with our HMM Library. Since, we already did pitch based representation,  \n",
    "let's do it for **Event Based Representation** (which is essentially denotes music as a sequence of events). So while pitch based representation  \n",
    "is between 0-128 unique pitch values, the event based representation is between 0-387 unique events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1 -342039.48315978             +nan\n",
      "         2 -247374.72829457  +94664.75486521\n",
      "         3 -245342.01670821   +2032.71158636\n",
      "         4 -241111.99778872   +4230.01891950\n",
      "         5 -234010.33003235   +7101.66775636\n",
      "         6 -226073.01600012   +7937.31403223\n",
      "         7 -219850.68997803   +6222.32602209\n",
      "         8 -214784.74736859   +5065.94260944\n",
      "         9 -210461.87431431   +4322.87305428\n",
      "        10 -206931.61295364   +3530.26136067\n"
     ]
    }
   ],
   "source": [
    "collection = []\n",
    "for music in my_dataset:\n",
    "    collection.append(muspy.to_event_representation(music))\n",
    "\n",
    "collection = [time_step[0] for seq in collection for time_step in seq]\n",
    "collection = np.array(collection)\n",
    "# # Initialize and fit the HMM model\n",
    "model = CategoricalHMM(n_components=hidden_states, verbose=True)\n",
    "model.fit(collection.reshape(-1,1))\n",
    "\n",
    "# Generate synthetic music sequence\n",
    "synthetic_sequence, _ = model.sample(n_samples=sythetic_music_sequence_length)\n",
    "# synthetic_sequence = synthetic_sequence[0]\n",
    "synthetic_music_event = muspy.from_event_representation(synthetic_sequence)\n",
    "muspy.write_midi('C:/LUMS/Spring 23-24/GenAI/PA2.2/event_based.mid', synthetic_music_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add some fun, lets take it up a notch and go for the __Note Based Representation__.  \n",
    "More on that here: \n",
    "1. https://muspy.readthedocs.io/en/v0.3.0/representations/note.html \n",
    "2. https://salu133445.github.io/muspy/classes/note.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** This is a bit tricky since we have 4 features per observation. We'll leave it to you to devise a way to deal with it.  \\\n",
    "There are alot of approaches which can be used. As some features are categorical, and some are continuous, hence you can try different HMMs types or a single HMM to rule them all. Just generate some good music. \\\n",
    "Before you start, do take a peek of the available HMM models in the HMMlearn library __(you are allowed to import additional models if you want)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here and save the sampled sequence as MIDI file, name it \"note_based.mid\"\n",
    "note_rep_collection = []\n",
    "for music in my_dataset:\n",
    "    note_rep_collection.append(muspy.to_note_representation(music))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
